{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g2KRK23NJX5i",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "outputId": "6a3f97e2-be2d-45f1-865e-3dd34b157d53"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at /tmp/ipython-input-1393109398.py:2 ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-765415322.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m \u001b[0;32mimport\u001b[0m  \u001b[0mSparkContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mxyz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallelize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/core/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[1;32m    203\u001b[0m             )\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m             self._do_init(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/core/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m                     \u001b[0;31m# Raise error if there is already a running Spark context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m                     raise ValueError(\n\u001b[0m\u001b[1;32m    458\u001b[0m                         \u001b[0;34m\"Cannot run multiple SparkContexts at once; \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m                         \u001b[0;34m\"existing SparkContext(app=%s, master=%s)\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at /tmp/ipython-input-1393109398.py:2 "
          ]
        }
      ],
      "source": [
        "from pyspark import  SparkContext\n",
        "sc = SparkContext()\n",
        "xyz = sc.parallelize([1,2,3,4])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(xyz))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZGxuGF1NKydJ",
        "outputId": "e5a70db3-e9e4-45d1-aead-c36c3ab04c6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pyspark.core.rdd.RDD'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xyz.collect()\n",
        "# very expensive for large dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Y7ndFj3LshQ",
        "outputId": "fbc05a83-da47-44f8-db4e-fe53d0db8950"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 2, 3, 4]"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xyz.take(2)\n",
        "# we can take this this will give only limited number of records"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rXWNVWdnQPym",
        "outputId": "b5174f7f-3894-4f24-b5b3-6776db7fbb78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 2]"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xyz.first()\n",
        "# this give only one record"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9dneOp4nQfY-",
        "outputId": "4b813ade-354e-4134-c071-19b8a75f980f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xyz.getNumPartitions()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W53k3VmOQnep",
        "outputId": "844476fb-5e5c-4ccf-ecaf-ac1fce7ace85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "abc  = xyz.coalesce(1)"
      ],
      "metadata": {
        "id": "iGV_5DkNRYgh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "abc.getNumPartitions()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5924pEsR1N1",
        "outputId": "548ccf57-5feb-48e4-b95a-8e9549521d3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rama = abc.repartition(4)"
      ],
      "metadata": {
        "id": "PkSFYHHvR6vT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rama.getNumPartitions()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IldjGzGfSTDD",
        "outputId": "6aab6dc0-daee-47a7-e625-bd587b953d03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7RFLDYUcJ1mw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = sc.parallelize(['ram','anil','anil','rakesh'])\n",
        "words.getNumPartitions(\n",
        ")\n",
        "words_filter  = words.filter(lambda x:'anil' in x )\n",
        "words_filter.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CFVmvXeXSWiE",
        "outputId": "964b6eb4-7443-4489-a9f4-589131cedd40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['anil', 'anil']"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "alpha = words.map(lambda x:(x,1))\n",
        "alpha"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LcbHMgPsVeYE",
        "outputId": "b95f0d2a-2782-478c-d6f8-7b2cb5ea3ede"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PythonRDD[127] at RDD at PythonRDD.scala:56"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(alpha)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hNSmSX35YvKe",
        "outputId": "73aca9ec-bdee-4969-f4fb-83d9a9474b07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PythonRDD[127] at RDD at PythonRDD.scala:56\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "alpha.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5L4S04ruYw6y",
        "outputId": "be75052a-ea29-47b6-c8cb-9a90cddf5c99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('ram', 1), ('anil', 1), ('anil', 1), ('rakesh', 1)]"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "myresult = alpha.reduceByKey(lambda x,y:(x+y))\n",
        "myresult.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gvJKKkTSY1EX",
        "outputId": "e7c784c0-2f54-43f6-da3a-5c4ede0b0fc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('ram', 1), ('anil', 2), ('rakesh', 1)]"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "myresult.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1685SHseZB8D",
        "outputId": "13b8afa7-bc5f-448e-bcb2-1ff3fa9ae7de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "myresult.coalesce(1).saveAsTextFile(\"/content/sample_data/output1.txt\")"
      ],
      "metadata": {
        "id": "OQirOSzgZHIx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "97f7d101-ee1e-4ab6-9062-863868e72ae6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "An error occurred while calling o651.saveAsTextFile.\n: org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory file:/content/sample_data/output1.txt already exists\n\tat org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:131)\n\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.assertConf(SparkHadoopWriter.scala:303)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:73)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1094)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:417)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1092)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1065)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:417)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1029)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1011)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:417)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1010)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$2(PairRDDFunctions.scala:967)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:417)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:965)\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$2(RDD.scala:1631)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:417)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1631)\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$1(RDD.scala:1617)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:417)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1617)\n\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile(JavaRDDLike.scala:565)\n\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile$(JavaRDDLike.scala:564)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:46)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3665318627.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmyresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoalesce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTextFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/sample_data/output1.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/core/rdd.py\u001b[0m in \u001b[0;36msaveAsTextFile\u001b[0;34m(self, path, compressionCodecClass)\u001b[0m\n\u001b[1;32m   3292\u001b[0m             \u001b[0mkeyed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTextFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompressionCodec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3293\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3294\u001b[0;31m             \u001b[0mkeyed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTextFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3295\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3296\u001b[0m     \u001b[0;31m# Pair functions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1362\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1363\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    328\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m                     format(target_id, \".\", name), value)\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o651.saveAsTextFile.\n: org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory file:/content/sample_data/output1.txt already exists\n\tat org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:131)\n\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.assertConf(SparkHadoopWriter.scala:303)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:73)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1094)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:417)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1092)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1065)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:417)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1029)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1011)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:417)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1010)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$2(PairRDDFunctions.scala:967)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:417)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:965)\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$2(RDD.scala:1631)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:417)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1631)\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$1(RDD.scala:1617)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:417)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1617)\n\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile(JavaRDDLike.scala:565)\n\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile$(JavaRDDLike.scala:564)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:46)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "abc = [\"my name is anil\",\"his home at bikaner\",\"his has\"]\n"
      ],
      "metadata": {
        "id": "UI4RK2WqZpI-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "abc = sc.textFile(\"/content/sample_data/war_and_peace.txt\")"
      ],
      "metadata": {
        "id": "9F2j7Ui8tV7z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "abc.take(5)"
      ],
      "metadata": {
        "id": "GND980wywboe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "abc.getNumPartitions()"
      ],
      "metadata": {
        "id": "t72LFxMVwfJw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# find the word count"
      ],
      "metadata": {
        "id": "AFP6ApaBwiV4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mycont = abc.flatMap(lambda x:x.split(\" \"))\n",
        "mycont.take(5)"
      ],
      "metadata": {
        "id": "NigFeVlTwqc9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rama = mycont.map(lambda x:(x,1))\n",
        "rama.take(5)"
      ],
      "metadata": {
        "id": "lTvnt4nSw8b4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_result = rama.reduceByKey(lambda x,y:(x+y))\n",
        "final_result.take(5)"
      ],
      "metadata": {
        "id": "fXzbvbj5xNQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_result.saveAsTextFile('/content/sample_data/output_war_and_peace.txt')"
      ],
      "metadata": {
        "id": "GdEzVYkFxU4H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_result.getNumPartitions()"
      ],
      "metadata": {
        "id": "Ek0081eazkCc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# how to read csv file in rdd"
      ],
      "metadata": {
        "id": "ni9asSpp2Hjs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mycont = sc.textFile('/content/sample_data/movies.csv')\n",
        "mycont.take(10)"
      ],
      "metadata": {
        "id": "QxY-x_jGzsxx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "-Zw3tjay1cc9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "myrat  = sc.textFile('/content/sample_data/ratings.csv')\n",
        "myrat.take(4)"
      ],
      "metadata": {
        "id": "NKovOhiJ1Tjv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "myrat.getNumPartitions()"
      ],
      "metadata": {
        "id": "6Bc5e8nQ1fOO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mycont.getNumPartitions()"
      ],
      "metadata": {
        "id": "PHZ-7l1E2OSe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "myrat.cache()"
      ],
      "metadata": {
        "id": "dokzaUfd2P6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "myrat.count(\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "omN2uAoy2wq1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ROW contains the name of columns also so we need to exclude that first row\n",
        "pure_data = mycont.filter(lambda line:not line.startswith('movieId'))\n",
        "pure_data.take(5 )"
      ],
      "metadata": {
        "id": "moNDAFBj2yi2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pure_rating = myrat.filter(lambda line: not line.startswith('userId'))\n",
        "pure_rating.take(5)"
      ],
      "metadata": {
        "id": "rgHUCgKW3S8d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "movies_by_moiveId = pure_data.map(lambda line :line.split(\",\")).map(lambda token:(int(token[0]),token[1]))\n",
        "movies_by_moiveId.first()"
      ],
      "metadata": {
        "id": "EthjPlEt4buv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# rdd dont know schema like data frame ,rdd know position"
      ],
      "metadata": {
        "id": "C8Mmbqe45Sl1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rating_by_userId = pure_rating.map(lambda line:line.split(\",\")).map(lambda token:(int(token[1]),float(token[2])))\n",
        "rating_by_userId.first()"
      ],
      "metadata": {
        "id": "MaHW-Wzk5yK-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# how to join\n",
        "movies_by_moiveId.join(rating_by_userId).first()\n"
      ],
      "metadata": {
        "id": "f0q7slL0564C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# find out top 10 movies based on the highesst average rating,consider only those movies that have got atleat 100 ratings\n"
      ],
      "metadata": {
        "id": "kIm98dHj7I3T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize spark\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName('practice').getOrCreate()\n",
        "# create dataframe from a list of tuples\n",
        "\n",
        "data = [(\"alice\",34),(\"bob\",45),(\"cathy\",29)]\n",
        "columns = [\"name\",\"age\"]\n",
        "df = spark.createDataFrame(data,columns)\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eb4ovxNrJN4h",
        "outputId": "193e1210-0113-40d3-bf0b-96905402a88f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+---+\n",
            "| name|age|\n",
            "+-----+---+\n",
            "|alice| 34|\n",
            "|  bob| 45|\n",
            "|cathy| 29|\n",
            "+-----+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "id": "hPJ_JUPNJrRZ",
        "outputId": "d01eafcc-a225-4548-d248-a3e437dab46d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pyspark.sql.classic.dataframe.DataFrame"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>pyspark.sql.classic.dataframe.DataFrame</b><br/>def __init__(jdf: &#x27;JavaObject&#x27;, sql_ctx: Union[&#x27;SQLContext&#x27;, &#x27;SparkSession&#x27;])</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.12/dist-packages/pyspark/sql/classic/dataframe.py</a>A distributed collection of data grouped into named columns.\n",
              "\n",
              ".. versionadded:: 1.3.0\n",
              "\n",
              ".. versionchanged:: 3.4.0\n",
              "    Supports Spark Connect.\n",
              "\n",
              "Examples\n",
              "--------\n",
              "A :class:`DataFrame` is equivalent to a relational table in Spark SQL,\n",
              "and can be created using various functions in :class:`SparkSession`:\n",
              "\n",
              "&gt;&gt;&gt; people = spark.createDataFrame([\n",
              "...     {&quot;deptId&quot;: 1, &quot;age&quot;: 40, &quot;name&quot;: &quot;Hyukjin Kwon&quot;, &quot;gender&quot;: &quot;M&quot;, &quot;salary&quot;: 50},\n",
              "...     {&quot;deptId&quot;: 1, &quot;age&quot;: 50, &quot;name&quot;: &quot;Takuya Ueshin&quot;, &quot;gender&quot;: &quot;M&quot;, &quot;salary&quot;: 100},\n",
              "...     {&quot;deptId&quot;: 2, &quot;age&quot;: 60, &quot;name&quot;: &quot;Xinrong Meng&quot;, &quot;gender&quot;: &quot;F&quot;, &quot;salary&quot;: 150},\n",
              "...     {&quot;deptId&quot;: 3, &quot;age&quot;: 20, &quot;name&quot;: &quot;Haejoon Lee&quot;, &quot;gender&quot;: &quot;M&quot;, &quot;salary&quot;: 200}\n",
              "... ])\n",
              "\n",
              "Once created, it can be manipulated using the various domain-specific-language\n",
              "(DSL) functions defined in: :class:`DataFrame`, :class:`Column`.\n",
              "\n",
              "To select a column from the :class:`DataFrame`, use the apply method:\n",
              "\n",
              "&gt;&gt;&gt; age_col = people.age\n",
              "\n",
              "A more concrete example:\n",
              "\n",
              "&gt;&gt;&gt; # To create DataFrame using SparkSession\n",
              "... department = spark.createDataFrame([\n",
              "...     {&quot;id&quot;: 1, &quot;name&quot;: &quot;PySpark&quot;},\n",
              "...     {&quot;id&quot;: 2, &quot;name&quot;: &quot;ML&quot;},\n",
              "...     {&quot;id&quot;: 3, &quot;name&quot;: &quot;Spark SQL&quot;}\n",
              "... ])\n",
              "\n",
              "&gt;&gt;&gt; people.filter(people.age &gt; 30).join(\n",
              "...     department, people.deptId == department.id).groupBy(\n",
              "...     department.name, &quot;gender&quot;).agg(\n",
              "...         {&quot;salary&quot;: &quot;avg&quot;, &quot;age&quot;: &quot;max&quot;}).sort(&quot;max(age)&quot;).show()\n",
              "+-------+------+-----------+--------+\n",
              "|   name|gender|avg(salary)|max(age)|\n",
              "+-------+------+-----------+--------+\n",
              "|PySpark|     M|       75.0|      50|\n",
              "|     ML|     F|      150.0|      60|\n",
              "+-------+------+-----------+--------+\n",
              "\n",
              "Notes\n",
              "-----\n",
              "A DataFrame should only be created as described above. It should not be directly\n",
              "created via using the constructor.</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 105);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.printSchema()"
      ],
      "metadata": {
        "id": "LJirhv8QJ8F3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# select specific columns\n",
        "\n",
        "df.select(\"name\").show()"
      ],
      "metadata": {
        "id": "Wvau_ZzqKMlm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mydf1 = df.select(\"name\")\n",
        "mydf1.show()"
      ],
      "metadata": {
        "id": "96VMPDu3K1ON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.filter(df.age>30).show()"
      ],
      "metadata": {
        "id": "sRb3_rZILZwj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## how to save the output dataframe to the hdfs?\n",
        "\n",
        "mydf1.write.csv(\"/content/sample_data/emp.csv\")"
      ],
      "metadata": {
        "id": "XTn2EMhbLycT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mydf1.rdd.getNumPartitions()"
      ],
      "metadata": {
        "id": "OkxcDW9gMKBS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# add the new colummn to existing dataframe\n",
        "df = df.withColumn(\"age_in_5_years\", df.age+5)"
      ],
      "metadata": {
        "id": "oNaE3kAIMUxS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.show()"
      ],
      "metadata": {
        "id": "9xA4xOd2NUYx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = [('hr',4000),('it',3453),('hr',343),('it',3432)]\n",
        "column = [\"dept\",\"salary\"]\n",
        "df2 = spark.createDataFrame(data,column)\n",
        "df2.show()"
      ],
      "metadata": {
        "id": "QccQYrX9NVqI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df3 = df2.groupby(\"dept\").count()"
      ],
      "metadata": {
        "id": "ZaAV6xWsOWY_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df3.write.json(\"/content/sample_data/play1.json\")"
      ],
      "metadata": {
        "id": "v3N1TWsZOb3m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## how to create frame for hdfs files(csv,json,orc,parquet,text)\n",
        "## mydf = spark.read.csv\n",
        "## mydf = spark.read.json\n",
        "\n",
        "mydf = spark.read.csv(\"/content/sample_data/movies.csv\",inferSchema=True,header = True)\n",
        "mydf.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qWun7fuBOefr",
        "outputId": "fad601f5-33b9-49b1-9c35-b00b3d9ca065"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+--------------------+--------------------+\n",
            "|movieId|               title|              genres|\n",
            "+-------+--------------------+--------------------+\n",
            "|      1|    Toy Story (1995)|Adventure|Animati...|\n",
            "|      2|      Jumanji (1995)|Adventure|Childre...|\n",
            "|      3|Grumpier Old Men ...|      Comedy|Romance|\n",
            "|      4|Waiting to Exhale...|Comedy|Drama|Romance|\n",
            "|      5|Father of the Bri...|              Comedy|\n",
            "|      6|         Heat (1995)|Action|Crime|Thri...|\n",
            "|      7|      Sabrina (1995)|      Comedy|Romance|\n",
            "|      8| Tom and Huck (1995)|  Adventure|Children|\n",
            "|      9| Sudden Death (1995)|              Action|\n",
            "|     10|    GoldenEye (1995)|Action|Adventure|...|\n",
            "|     11|American Presiden...|Comedy|Drama|Romance|\n",
            "|     12|Dracula: Dead and...|       Comedy|Horror|\n",
            "|     13|        Balto (1995)|Adventure|Animati...|\n",
            "|     14|        Nixon (1995)|               Drama|\n",
            "|     15|Cutthroat Island ...|Action|Adventure|...|\n",
            "|     16|       Casino (1995)|         Crime|Drama|\n",
            "|     17|Sense and Sensibi...|       Drama|Romance|\n",
            "|     18|   Four Rooms (1995)|              Comedy|\n",
            "|     19|Ace Ventura: When...|              Comedy|\n",
            "|     20|  Money Train (1995)|Action|Comedy|Cri...|\n",
            "+-------+--------------------+--------------------+\n",
            "only showing top 20 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## retrive all movie id ,movie name\n",
        "mydf.select(\"movieId\",\"title\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W_CWmVEGQqAC",
        "outputId": "d19bfc43-e3d0-4232-9160-05360d9046f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+--------------------+\n",
            "|movieId|               title|\n",
            "+-------+--------------------+\n",
            "|      1|    Toy Story (1995)|\n",
            "|      2|      Jumanji (1995)|\n",
            "|      3|Grumpier Old Men ...|\n",
            "|      4|Waiting to Exhale...|\n",
            "|      5|Father of the Bri...|\n",
            "|      6|         Heat (1995)|\n",
            "|      7|      Sabrina (1995)|\n",
            "|      8| Tom and Huck (1995)|\n",
            "|      9| Sudden Death (1995)|\n",
            "|     10|    GoldenEye (1995)|\n",
            "|     11|American Presiden...|\n",
            "|     12|Dracula: Dead and...|\n",
            "|     13|        Balto (1995)|\n",
            "|     14|        Nixon (1995)|\n",
            "|     15|Cutthroat Island ...|\n",
            "|     16|       Casino (1995)|\n",
            "|     17|Sense and Sensibi...|\n",
            "|     18|   Four Rooms (1995)|\n",
            "|     19|Ace Ventura: When...|\n",
            "|     20|  Money Train (1995)|\n",
            "+-------+--------------------+\n",
            "only showing top 20 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## retive the movie name where movieid >=5\n",
        "\n",
        "mydf.filter(mydf.movieId>=5).select(\"movieId\",\"title\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vityS0F0RMv0",
        "outputId": "96fddf1c-66dc-4a03-a1da-cfd8dc505ffd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+--------------------+\n",
            "|movieId|               title|\n",
            "+-------+--------------------+\n",
            "|      5|Father of the Bri...|\n",
            "|      6|         Heat (1995)|\n",
            "|      7|      Sabrina (1995)|\n",
            "|      8| Tom and Huck (1995)|\n",
            "|      9| Sudden Death (1995)|\n",
            "|     10|    GoldenEye (1995)|\n",
            "|     11|American Presiden...|\n",
            "|     12|Dracula: Dead and...|\n",
            "|     13|        Balto (1995)|\n",
            "|     14|        Nixon (1995)|\n",
            "|     15|Cutthroat Island ...|\n",
            "|     16|       Casino (1995)|\n",
            "|     17|Sense and Sensibi...|\n",
            "|     18|   Four Rooms (1995)|\n",
            "|     19|Ace Ventura: When...|\n",
            "|     20|  Money Train (1995)|\n",
            "|     21|   Get Shorty (1995)|\n",
            "|     22|      Copycat (1995)|\n",
            "|     23|    Assassins (1995)|\n",
            "|     24|       Powder (1995)|\n",
            "+-------+--------------------+\n",
            "only showing top 20 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# total number of movies by movieid wise?\n",
        "mydf.groupBy(\"movieId\").count().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JD5JDdyRRiN2",
        "outputId": "08d86523-b803-4778-d2c0-e2baa6bdcebe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----+\n",
            "|movieId|count|\n",
            "+-------+-----+\n",
            "|    148|    1|\n",
            "|    471|    1|\n",
            "|    496|    1|\n",
            "|    833|    1|\n",
            "|   1088|    1|\n",
            "|   1238|    1|\n",
            "|   1342|    1|\n",
            "|   1580|    1|\n",
            "|   1591|    1|\n",
            "|   1645|    1|\n",
            "|   1829|    1|\n",
            "|   1959|    1|\n",
            "|   2122|    1|\n",
            "|   2142|    1|\n",
            "|   2366|    1|\n",
            "|   2659|    1|\n",
            "|   2866|    1|\n",
            "|   3175|    1|\n",
            "|   3794|    1|\n",
            "|   3918|    1|\n",
            "+-------+-----+\n",
            "only showing top 20 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## how to create dataframe :json files of hdfs\n",
        "mydf = spark.read.json(\"/content/sample_data/sales.json\")\n",
        "mydf.show()"
      ],
      "metadata": {
        "id": "Hh1JANCJRwvU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ratings = spark.read.csv(\"/content/sample_data/ratings.csv\",inferSchema=True,header=True)\n",
        "ratings.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FU6Vro0dd4O0",
        "outputId": "8c670c76-2fe6-412b-922d-ab60eaa4e93e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-------+------+---------+\n",
            "|userId|movieId|rating|timestamp|\n",
            "+------+-------+------+---------+\n",
            "|     1|      1|   4.0|964982703|\n",
            "|     1|      3|   4.0|964981247|\n",
            "|     1|      6|   4.0|964982224|\n",
            "|     1|     47|   5.0|964983815|\n",
            "|     1|     50|   5.0|964982931|\n",
            "|     1|     70|   3.0|964982400|\n",
            "|     1|    101|   5.0|964980868|\n",
            "|     1|    110|   4.0|964982176|\n",
            "|     1|    151|   5.0|964984041|\n",
            "|     1|    157|   5.0|964984100|\n",
            "|     1|    163|   5.0|964983650|\n",
            "|     1|    216|   5.0|964981208|\n",
            "|     1|    223|   3.0|964980985|\n",
            "|     1|    231|   5.0|964981179|\n",
            "|     1|    235|   4.0|964980908|\n",
            "|     1|    260|   5.0|964981680|\n",
            "|     1|    296|   3.0|964982967|\n",
            "|     1|    316|   3.0|964982310|\n",
            "|     1|    333|   5.0|964981179|\n",
            "|     1|    349|   4.0|964982563|\n",
            "+------+-------+------+---------+\n",
            "only showing top 20 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "movies = spark.read.csv(\"/content/sample_data/movies.csv\",inferSchema=True,header=True)\n",
        "movies.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I3EDUsPFd95V",
        "outputId": "e13b705d-0e87-4f21-ab62-4ec6199d1207"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+--------------------+--------------------+\n",
            "|movieId|               title|              genres|\n",
            "+-------+--------------------+--------------------+\n",
            "|      1|    Toy Story (1995)|Adventure|Animati...|\n",
            "|      2|      Jumanji (1995)|Adventure|Childre...|\n",
            "|      3|Grumpier Old Men ...|      Comedy|Romance|\n",
            "|      4|Waiting to Exhale...|Comedy|Drama|Romance|\n",
            "|      5|Father of the Bri...|              Comedy|\n",
            "|      6|         Heat (1995)|Action|Crime|Thri...|\n",
            "|      7|      Sabrina (1995)|      Comedy|Romance|\n",
            "|      8| Tom and Huck (1995)|  Adventure|Children|\n",
            "|      9| Sudden Death (1995)|              Action|\n",
            "|     10|    GoldenEye (1995)|Action|Adventure|...|\n",
            "|     11|American Presiden...|Comedy|Drama|Romance|\n",
            "|     12|Dracula: Dead and...|       Comedy|Horror|\n",
            "|     13|        Balto (1995)|Adventure|Animati...|\n",
            "|     14|        Nixon (1995)|               Drama|\n",
            "|     15|Cutthroat Island ...|Action|Adventure|...|\n",
            "|     16|       Casino (1995)|         Crime|Drama|\n",
            "|     17|Sense and Sensibi...|       Drama|Romance|\n",
            "|     18|   Four Rooms (1995)|              Comedy|\n",
            "|     19|Ace Ventura: When...|              Comedy|\n",
            "|     20|  Money Train (1995)|Action|Comedy|Cri...|\n",
            "+-------+--------------------+--------------------+\n",
            "only showing top 20 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_table = movies.join(ratings,movies.movieId==ratings.movieId)\n",
        "final_table.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "icI9QLvyeG9l",
        "outputId": "e7b44260-abb2-4b39-aec3-d2dcbb6721c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+--------------------+--------------------+------+-------+------+---------+\n",
            "|movieId|               title|              genres|userId|movieId|rating|timestamp|\n",
            "+-------+--------------------+--------------------+------+-------+------+---------+\n",
            "|      1|    Toy Story (1995)|Adventure|Animati...|     1|      1|   4.0|964982703|\n",
            "|      3|Grumpier Old Men ...|      Comedy|Romance|     1|      3|   4.0|964981247|\n",
            "|      6|         Heat (1995)|Action|Crime|Thri...|     1|      6|   4.0|964982224|\n",
            "|     47|Seven (a.k.a. Se7...|    Mystery|Thriller|     1|     47|   5.0|964983815|\n",
            "|     50|Usual Suspects, T...|Crime|Mystery|Thr...|     1|     50|   5.0|964982931|\n",
            "|     70|From Dusk Till Da...|Action|Comedy|Hor...|     1|     70|   3.0|964982400|\n",
            "|    101|Bottle Rocket (1996)|Adventure|Comedy|...|     1|    101|   5.0|964980868|\n",
            "|    110|   Braveheart (1995)|    Action|Drama|War|     1|    110|   4.0|964982176|\n",
            "|    151|      Rob Roy (1995)|Action|Drama|Roma...|     1|    151|   5.0|964984041|\n",
            "|    157|Canadian Bacon (1...|          Comedy|War|     1|    157|   5.0|964984100|\n",
            "|    163|    Desperado (1995)|Action|Romance|We...|     1|    163|   5.0|964983650|\n",
            "|    216|Billy Madison (1995)|              Comedy|     1|    216|   5.0|964981208|\n",
            "|    223|       Clerks (1994)|              Comedy|     1|    223|   3.0|964980985|\n",
            "|    231|Dumb & Dumber (Du...|    Adventure|Comedy|     1|    231|   5.0|964981179|\n",
            "|    235|      Ed Wood (1994)|        Comedy|Drama|     1|    235|   4.0|964980908|\n",
            "|    260|Star Wars: Episod...|Action|Adventure|...|     1|    260|   5.0|964981680|\n",
            "|    296| Pulp Fiction (1994)|Comedy|Crime|Dram...|     1|    296|   3.0|964982967|\n",
            "|    316|     Stargate (1994)|Action|Adventure|...|     1|    316|   3.0|964982310|\n",
            "|    333|    Tommy Boy (1995)|              Comedy|     1|    333|   5.0|964981179|\n",
            "|    349|Clear and Present...|Action|Crime|Dram...|     1|    349|   4.0|964982563|\n",
            "+-------+--------------------+--------------------+------+-------+------+---------+\n",
            "only showing top 20 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## find out top 10 movies on the highest avg rating,consider only those movies that have got at least 100 rating\n",
        "from pyspark.sql.functions import avg, count, desc\n",
        "\n",
        "result = (\n",
        "    ratings\n",
        "    .groupBy(\"movieId\")\n",
        "    .agg(\n",
        "        avg(\"rating\").alias(\"avg_rating\"),\n",
        "        count(\"rating\").alias(\"rating_count\")\n",
        "    )\n",
        "    .filter(\"rating_count >= 100\")\n",
        "    .orderBy(desc(\"avg_rating\"))\n",
        "    .limit(10)\n",
        ")\n",
        "\n",
        "result.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_RXaUtuSepgP",
        "outputId": "c1d7d5f6-5ec9-4989-8f06-acd28f437826"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----------------+------------+\n",
            "|movieId|       avg_rating|rating_count|\n",
            "+-------+-----------------+------------+\n",
            "|    318|4.429022082018927|         317|\n",
            "|    858|        4.2890625|         192|\n",
            "|   2959|4.272935779816514|         218|\n",
            "|   1221| 4.25968992248062|         129|\n",
            "|  48516|4.252336448598131|         107|\n",
            "|   1213|             4.25|         126|\n",
            "|    912|             4.24|         100|\n",
            "|  58559|4.238255033557047|         149|\n",
            "|     50|4.237745098039215|         204|\n",
            "|   1197|4.232394366197183|         142|\n",
            "+-------+-----------------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X-ranaFYfHh3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}